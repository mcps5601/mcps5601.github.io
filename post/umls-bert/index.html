<!DOCTYPE html>















<html lang="zh-tw">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>UmlsBERT - 隨英紛飛</title>

  
  
  <meta name="description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  " />
  <meta name="author" content="" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://mcps5601.github.io/app.min.css" />

  

  
  <link rel="preload" as="image" href="https://mcps5601.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://mcps5601.github.io/twitter.svg" />
  
  <link rel="preload" as="image" href="https://mcps5601.github.io/github.svg" />
  

  
  <link rel="icon" href="https://mcps5601.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://mcps5601.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.88.1" />

  
  

  
  
  
  
  
  
  
  <meta property="og:title" content="UmlsBERT" />
<meta property="og:description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mcps5601.github.io/post/umls-bert/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-08-07T11:54:01+08:00" />
<meta property="article:modified_time" content="2022-08-07T11:54:01+08:00" />


  
  <meta itemprop="name" content="UmlsBERT">
<meta itemprop="description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  "><meta itemprop="datePublished" content="2022-08-07T11:54:01+08:00" />
<meta itemprop="dateModified" content="2022-08-07T11:54:01+08:00" />
<meta itemprop="wordCount" content="138">
<meta itemprop="keywords" content="論文,NLP,BERT,生醫," />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="UmlsBERT"/>
<meta name="twitter:description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  "/>

  
  
</head>


  <body class="not-ready" data-menu="false">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://mcps5601.github.io/">隨英紛飛</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  

  
  <nav class="social">
    
    <a
      class="twitter"
      style="--url: url(./twitter.svg)"
      href="https://twitter.com/mcps5601"
      target="_blank"
    ></a>
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/mcps5601"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      <time>Sunday, August 7, 2022</time>
      
    </p>
    <h1>UmlsBERT</h1>
  </header>
  <section class="post-content"><h2 id="info">Info</h2>
<ul>
<li>會議: NAACL 2021 (<a href="https://aclanthology.org/2021.naacl-main.139.pdf">論文連結</a>)(<a href="https://github.com/gmichalo/UmlsBERT">code連結</a>)</li>
<li>作者單位: University of Waterloo</li>
</ul>
<h2 id="研究動機">研究動機</h2>
<p><a href="https://academic.oup.com/bioinformatics/article/36/4/1234/5566506">BioBERT</a> 跟 <a href="https://aclanthology.org/W19-1909/">Clinical BERT</a> 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識</p>
<h2 id="主要方法">主要方法</h2>
<p>使用 MIMIC-III <code>NOTEEVENTS</code> 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識</p>
<h3 id="umls-semantic-types">UMLS semantic types</h3>
<ul>
<li>BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 <strong>semantic type embeddings</strong>:
$$
ST^\top s_w
$$</li>
<li>$ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)
<ul>
<li>$D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數</li>
<li>$d$: Transformer layer 的 hidden size</li>
</ul>
</li>
<li>$s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)
<ul>
<li>其中每一個 item 代表一種 semantic type</li>
<li>如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0</li>
</ul>
</li>
</ul>
<h3 id="mlm">MLM</h3>
<ul>
<li>MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法
<img src="https://i.imgur.com/V28cEyZ.png" alt=""></li>
</ul>
<h2 id="重要發現">重要發現</h2>
<ul>
<li>UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3)</li>
<li>UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)</li>
</ul>
<h2 id="創新的部份">創新的部份</h2>
<ul>
<li>運用 UMLS domain knowledge</li>
</ul>
<h2 id="缺點">缺點</h2>
<ul>
<li>沒有與 <a href="https://arxiv.org/abs/1906.05474">BlueBERT</a> 比較</li>
<li>實驗著重在 NER 的部份</li>
<li>找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差</li>
</ul>
<h2 id="我的問題">我的問題</h2>
<ul>
<li>不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數</li>
</ul>
</section>

  
  
  <footer class="post-tags">
     
    <a href="https://mcps5601.github.io/tags/%E8%AB%96%E6%96%87">論文</a>
     
    <a href="https://mcps5601.github.io/tags/nlp">NLP</a>
     
    <a href="https://mcps5601.github.io/tags/bert">BERT</a>
     
    <a href="https://mcps5601.github.io/tags/%E7%94%9F%E9%86%AB">生醫</a>
    
  </footer>
  

  
  
  
  <nav class="post-nav">
     
    <a class="next" href="https://mcps5601.github.io/post/graph_ch_2_1/"><span>Trees and Distance: Basic Properties</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://mcps5601.github.io/">隨英紛飛</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

    <script>
    MathJax = {
        tex: {
            inlineMath: [["$", "$"]],
        },
        displayMath: [
            ["$$", "$$"],
            ["\[\[", "\]\]"],
        ],
        svg: {
            fontCache: "global",
        },
    };
</script>
<script src="https://polyfill.io/v3/polyfill.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </body>
</html>
