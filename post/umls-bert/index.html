<!DOCTYPE html>


































<html
  class="not-ready text-sm lg:text-base"
  style="--bg: #faf6f1"
  lang="en"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>UmlsBERT - 隨英紛飛</title>

  
  <meta name="theme-color" />

  
  
  
  <meta name="description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  " />
  <meta name="author" content="隨英紛飛" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://mcps5601.github.io/main.min.css" />

  
  <script
    defer
    src="https://mcps5601.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
   
  <link rel="preload" as="image" href="https://mcps5601.github.io/theme.png" />

  
  
  
  

  
  <link rel="preload" as="image" href="https://mcps5601.github.io/scholar.svg" />
  
  <link rel="preload" as="image" href="https://mcps5601.github.io/twitter.svg" />
  
  <link rel="preload" as="image" href="https://mcps5601.github.io/github.svg" />
  

  
  <link rel="icon" href="https://mcps5601.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://mcps5601.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.88.1" />

  
  

  
  
  
  
  
  
  
  <meta property="og:title" content="UmlsBERT" />
<meta property="og:description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mcps5601.github.io/post/umls-bert/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-08-07T11:54:01+08:00" />
<meta property="article:modified_time" content="2022-08-07T11:54:01+08:00" />


  
  <meta itemprop="name" content="UmlsBERT">
<meta itemprop="description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  "><meta itemprop="datePublished" content="2022-08-07T11:54:01+08:00" />
<meta itemprop="dateModified" content="2022-08-07T11:54:01+08:00" />
<meta itemprop="wordCount" content="138">
<meta itemprop="keywords" content="論文,NLP,BERT,生醫," />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="UmlsBERT"/>
<meta name="twitter:description" content="Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  "/>

  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold"
      href="https://mcps5601.github.io/"
      >隨英紛飛</a
    >
    <div
      class="btn-dark text-[0] ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = `"#faf6f1"`.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./scholar.svg)"
        href="https://scholar.google.com.tw/citations?user=TM4JxJkAAAAJ&hl=zh-TW&oi=ao"
        target="_blank"
        rel="me"
      >
        scholar
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./twitter.svg)"
        href="https://twitter.com/mcps5601"
        target="_blank"
        rel="me"
      >
        twitter
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/mcps5601"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-16 pb-24 dark:prose-invert"
    >
      

<article>
  <header class="mb-20">
    <h1 class="!my-0 pb-2.5">UmlsBERT</h1>

    
    <div class="text-sm opacity-60">
      
      <time>Aug 7, 2022</time>
      
      
      
      
    </div>
    
  </header>

  <section><h2 id="info">Info</h2>
<ul>
<li>會議: NAACL 2021 (<a href="https://aclanthology.org/2021.naacl-main.139.pdf">論文連結</a>)(<a href="https://github.com/gmichalo/UmlsBERT">code連結</a>)</li>
<li>作者單位: University of Waterloo</li>
</ul>
<h2 id="研究動機">研究動機</h2>
<p><a href="https://academic.oup.com/bioinformatics/article/36/4/1234/5566506">BioBERT</a> 跟 <a href="https://aclanthology.org/W19-1909/">Clinical BERT</a> 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識</p>
<h2 id="主要方法">主要方法</h2>
<p>使用 MIMIC-III <code>NOTEEVENTS</code> 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識</p>
<h3 id="umls-semantic-types">UMLS semantic types</h3>
<ul>
<li>BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 <strong>semantic type embeddings</strong>:
$$
ST^\top s_w
$$</li>
<li>$ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)
<ul>
<li>$D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數</li>
<li>$d$: Transformer layer 的 hidden size</li>
</ul>
</li>
<li>$s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)
<ul>
<li>其中每一個 item 代表一種 semantic type</li>
<li>如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0</li>
</ul>
</li>
</ul>
<h3 id="mlm">MLM</h3>
<ul>
<li>MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法
<img src="https://i.imgur.com/V28cEyZ.png" alt=""></li>
</ul>
<h2 id="重要發現">重要發現</h2>
<ul>
<li>UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3)</li>
<li>UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)</li>
</ul>
<h2 id="創新的部份">創新的部份</h2>
<ul>
<li>運用 UMLS domain knowledge</li>
</ul>
<h2 id="缺點">缺點</h2>
<ul>
<li>沒有與 <a href="https://arxiv.org/abs/1906.05474">BlueBERT</a> 比較</li>
<li>實驗著重在 NER 的部份</li>
<li>找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差</li>
</ul>
<h2 id="我的問題">我的問題</h2>
<ul>
<li>不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數</li>
</ul>
</section>

  
  
  <footer class="mt-12 flex flex-wrap">
     
    <a
      class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]"
      href="https://mcps5601.github.io/tags/%E8%AB%96%E6%96%87"
      >論文</a
    >
     
    <a
      class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]"
      href="https://mcps5601.github.io/tags/nlp"
      >NLP</a
    >
     
    <a
      class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]"
      href="https://mcps5601.github.io/tags/bert"
      >BERT</a
    >
     
    <a
      class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]"
      href="https://mcps5601.github.io/tags/%E7%94%9F%E9%86%AB"
      >生醫</a
    >
    
  </footer>
  

  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://mcps5601.github.io/post/graph_ch_2_1/"
      ><span>Trees and Distance: Basic Properties</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  

  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2023
    <a class="link" href="https://mcps5601.github.io">隨英紛飛</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >Theme Paper</a
  >
</footer>

  </body>
</html>
