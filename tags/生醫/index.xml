<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>生醫 on Ying-Jia Lin</title>
    <link>https://yingjialin.org/tags/%E7%94%9F%E9%86%AB/</link>
    <description>Recent content in 生醫 on Ying-Jia Lin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2008–2019, Steve Francia and the lee.so; all rights reserved.</copyright>
    <lastBuildDate>Sun, 07 Aug 2022 11:54:01 +0800</lastBuildDate><atom:link href="https://yingjialin.org/tags/%E7%94%9F%E9%86%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UmlsBERT</title>
      <link>https://yingjialin.org/post/umls-bert/</link>
      <pubDate>Sun, 07 Aug 2022 11:54:01 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/umls-bert/</guid>
      <description>Info  會議: NAACL 2021 (論文連結)(code連結) 作者單位: University of Waterloo  研究動機 BioBERT 跟 Clinical BERT 在訓練階段並沒有利用到如 UMLS(Unified Medical Language System) 這種專家知識
主要方法 使用 MIMIC-III NOTEEVENTS 來預訓練 BERT，搭配以下兩種方法來使用 UMLS 知識
UMLS semantic types  BERT 的輸入除了原本三種 embeddings (token embeddings, positional embeddings, segment embeddings)，本篇還加入了 semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (基於MIMIC-III的) 字典中所可以對應到的 UMLS semantic types 總數 $d$: Transformer layer 的 hidden size   $s_w$: 任意一個字 $w$ 的 one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  其中每一個 item 代表一種 semantic type 如果 $w$ 完全對應不到 UMLS 的 semantic type 則 $s_w$ 全為 0    MLM  MLM 的目標改為多類別，即模型也必須要預測出與目標字有相同 CUI 的其他寫法   重要發現  UmlsBERT 可以提高 MEdNLI 跟其他4個 NER 資料集的表現 (Table 3) UmlsBERT 的預訓練過程可以使模型更能區分出不同的 UMLS semantic types (Figure 3)  創新的部份  運用 UMLS domain knowledge  缺點  沒有與 BlueBERT 比較 實驗著重在 NER 的部份 找出相近醫學字的實驗中 (Table 4)，BERT 並沒有表現特別差  我的問題  不知道 semantic embedding matrix $ST$ 如何初始化，以及 $ST$ 是否為可訓練之參數  </description>
    </item>
    
  </channel>
</rss>
