<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ying-Jia Lin</title>
    <link>https://yingjialin.org/</link>
    <description>Recent content on Ying-Jia Lin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright Â© 2008â€“2019, Steve Francia and the lee.so; all rights reserved.</copyright>
    <lastBuildDate>Sun, 07 Aug 2022 11:54:01 +0800</lastBuildDate><atom:link href="https://yingjialin.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ying-Jia Lin (æ—è‹±å˜‰)</title>
      <link>https://yingjialin.org/about/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://yingjialin.org/about/</guid>
      <description>Assistant Professor at Department of Artificial Intelligence, College of Intelligent Computing, Chang Gung University Email: yjlin@cgu.edu.tw
Office: B1412, Management Building (14-th floor), No 259, Wenhua 1st Rd, Guishan Dist, Taoyuan City 33302, Taiwan.
ğŸ’¡Prospective students are welcome!! Please contact me via email.
  Hello, I&#39;m Ying-Jia Lin. I am an Assistant Professor at the Department of Artificial Intelligence, College of Intelligent Computing (CoIC), Chang Gung University. Before joining CoIC, I was a postdoctoral researcher at the Department of Computer Science, National Tsing Hua University under the supervision of my PhD advisor, Prof.</description>
    </item>
    
    <item>
      <title>UmlsBERT</title>
      <link>https://yingjialin.org/post/umls-bert/</link>
      <pubDate>Sun, 07 Aug 2022 11:54:01 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/umls-bert/</guid>
      <description>Info  æœƒè­°: NAACL 2021 (è«–æ–‡é€£çµ)(codeé€£çµ) ä½œè€…å–®ä½: University of Waterloo  ç ”ç©¶å‹•æ©Ÿ BioBERT è·Ÿ Clinical BERT åœ¨è¨“ç·´éšæ®µä¸¦æ²’æœ‰åˆ©ç”¨åˆ°å¦‚ UMLS(Unified Medical Language System) é€™ç¨®å°ˆå®¶çŸ¥è­˜
ä¸»è¦æ–¹æ³• ä½¿ç”¨ MIMIC-III NOTEEVENTS ä¾†é è¨“ç·´ BERTï¼Œæ­é…ä»¥ä¸‹å…©ç¨®æ–¹æ³•ä¾†ä½¿ç”¨ UMLS çŸ¥è­˜
UMLS semantic types  BERT çš„è¼¸å…¥é™¤äº†åŸæœ¬ä¸‰ç¨® embeddings (token embeddings, positional embeddings, segment embeddings)ï¼Œæœ¬ç¯‡é‚„åŠ å…¥äº† semantic type embeddings: $$ ST^\top s_w $$ $ST$: semantic embedding matrix ($ST \in\mathbb{R}^{D_s\times d}$)  $D_s$: (åŸºæ–¼MIMIC-IIIçš„) å­—å…¸ä¸­æ‰€å¯ä»¥å°æ‡‰åˆ°çš„ UMLS semantic types ç¸½æ•¸ $d$: Transformer layer çš„ hidden size   $s_w$: ä»»æ„ä¸€å€‹å­— $w$ çš„ one-hot semantic vector ($s_w\in \mathbb{R}^{D_s}$)  å…¶ä¸­æ¯ä¸€å€‹ item ä»£è¡¨ä¸€ç¨® semantic type å¦‚æœ $w$ å®Œå…¨å°æ‡‰ä¸åˆ° UMLS çš„ semantic type å‰‡ $s_w$ å…¨ç‚º 0    MLM  MLM çš„ç›®æ¨™æ”¹ç‚ºå¤šé¡åˆ¥ï¼Œå³æ¨¡å‹ä¹Ÿå¿…é ˆè¦é æ¸¬å‡ºèˆ‡ç›®æ¨™å­—æœ‰ç›¸åŒ CUI çš„å…¶ä»–å¯«æ³•   é‡è¦ç™¼ç¾  UmlsBERT å¯ä»¥æé«˜ MEdNLI è·Ÿå…¶ä»–4å€‹ NER è³‡æ–™é›†çš„è¡¨ç¾ (Table 3) UmlsBERT çš„é è¨“ç·´éç¨‹å¯ä»¥ä½¿æ¨¡å‹æ›´èƒ½å€åˆ†å‡ºä¸åŒçš„ UMLS semantic types (Figure 3)  å‰µæ–°çš„éƒ¨ä»½  é‹ç”¨ UMLS domain knowledge  ç¼ºé»  æ²’æœ‰èˆ‡ BlueBERT æ¯”è¼ƒ å¯¦é©—è‘—é‡åœ¨ NER çš„éƒ¨ä»½ æ‰¾å‡ºç›¸è¿‘é†«å­¸å­—çš„å¯¦é©—ä¸­ (Table 4)ï¼ŒBERT ä¸¦æ²’æœ‰è¡¨ç¾ç‰¹åˆ¥å·®  æˆ‘çš„å•é¡Œ  ä¸çŸ¥é“ semantic embedding matrix $ST$ å¦‚ä½•åˆå§‹åŒ–ï¼Œä»¥åŠ $ST$ æ˜¯å¦ç‚ºå¯è¨“ç·´ä¹‹åƒæ•¸  </description>
    </item>
    
    <item>
      <title>Trees and Distance: Basic Properties</title>
      <link>https://yingjialin.org/post/graph_ch_2_1/</link>
      <pubDate>Mon, 15 Nov 2021 14:49:12 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/graph_ch_2_1/</guid>
      <description>åƒè€ƒæ›¸ç±: Introduction to Graph Theory - Second edition  æ¨¹åŸºæœ¬å®šç¾© acyclic  A graph with no cycle  forest  An acyclic graph  tree  A connected acyclic graph  leaf  A vertex of degree 1 åˆç¨±ä½œ pendant vertex  spanning subgraph  A spanning subgraph of $G$ is a subgraph with vertex set $V(G)$.  spanning tree  A spanning tree is a spanning subgraph that is a tree.</description>
    </item>
    
    <item>
      <title>RadBERT-CL: Factually-Aware Contrastive Learning For Radiology Report Classification</title>
      <link>https://yingjialin.org/post/radbert-cl/</link>
      <pubDate>Mon, 15 Nov 2021 14:38:40 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/radbert-cl/</guid>
      <description>è«–æ–‡è³‡è¨Š  æœƒè­°: Machine Learning for Health (ML4H) 2021 (è«–æ–‡é€£çµ) ä½œè€…å–®ä½: The University of Texas at Austin  ç ”ç©¶å‹•æ©Ÿ  BERT è·Ÿ BlueBert æ²’è¾¦æ³•æŠŠ uncertainty è·Ÿ negation çš„å•é¡Œè™•ç†å¾—å¾ˆå¥½  æœ¬ç¯‡é‡é»  Data augmentation  å› ç‚ºè¦åš contrastive pre-training learning   æ¥µåº¦ä»°è³´ Wu et al. (2020a)  Related work  Fang and Xie (2020) é‡å°æ­£å‘è³‡æ–™ä½¿ç”¨å¥å­å±¤ç´šçš„ back-translation Wu et al. (2020a) æ•´åˆ4ç¨®å¥å­å±¤ç´šçš„è³‡æ–™æ“´å¢  Word deletion Span deletion Reordering Synonym substitution    æ–¹æ³•  åœ¨ pre-training çš„éƒ¨åˆ†é€²è¡Œæ”¹é€²  åˆ©ç”¨ Contrastive pre-training ä¾†å¢åŠ æ¨¡å‹åœ¨ä»»å‹™ä¸Šçš„äº‹å¯¦ç¢ºèªèƒ½åŠ›   è³‡æ–™: radiology reports  MIMIC-CXR (Johnson et al.</description>
    </item>
    
    <item>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
      <link>https://yingjialin.org/post/sbert/</link>
      <pubDate>Thu, 07 Oct 2021 22:25:52 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/sbert/</guid>
      <description>è«–æ–‡è³‡è¨Š  æœƒè­°: EMNLP 2019 (è«–æ–‡é€£çµ)(codeé€£çµ) ä½œè€…å–®ä½: UKPLab ä¸»è¦ä»»å‹™: semantic textual similarity (STS)  å¿«é€Ÿé‡é»  é€™æ˜¯ä¸€å€‹ sentence embeddings çš„å·¥ä½œ æå‡º SBERT å’Œ SRoBERTa  åŸºæ–¼ siamese å’Œ triplet loss æ¶æ§‹   åœ¨ NLI è³‡æ–™é›†ä¸Šé€²è¡Œ fine-tuningï¼Œè¡¨ç¾æ¯” InferSentã€Universal Sentence Encoder é‚„è¦å¥½ åœ¨ STS è³‡æ–™é›†ä¸Šè´é InferSent 11.7 åˆ†ã€è´é Universal Sentence Encoder 5.5 åˆ† åœ¨ SentEval çš„å…©å€‹ä»»å‹™ä¸Šå¾—åˆ° 2.1 å’Œ 2.6 åˆ†çš„æå‡  èƒŒæ™¯ BERT  å°‡å…©å€‹å¥å­ä»¥SEPåˆèµ·ä¾†ç•¶æˆ1å€‹å¥å­ åœ¨ inference æ™‚æœƒå¾ˆæ²’æœ‰æ•ˆç‡  inference æ™‚æ˜¯è¦æ‰¾åˆ°æœ€ä½³çš„é…å°çµ„åˆ å‡è¨­æœ‰10,000å€‹å¥å­ï¼ŒBERT å°±è¦åš n*(n-1)/2 æ¬¡ï¼Œå…± 49,995,000 æ¬¡  10,000å€‹å¥å­åªæœƒæ‰¾æœ¬èº«ä»¥å¤–çš„10,000-1å€‹å¥å­ å› ç‚ºæ˜¯æ‰¾æˆå°å¥å­ï¼Œæ‰€ä»¥ç¸½æ¬¡æ•¸é™¤ä»¥2   ä»¥ V100 ä¾†åšé‹ç®—çš„è©±éœ€è¦èŠ±è²» 65 å°æ™‚    æ–¹æ³• æ¨¡å‹  å° BERT/ RoBERTa çš„è¼¸å‡ºä½¿ç”¨æ± åŒ– (pooling) çš„æ–¹æ³•é€²è¡Œè™•ç†: (1) å– CLS token (2) å– MEAN (3) å– MAX ä¾†å¾—åˆ°å›ºå®šå¤§å° (fixed sized) çš„å¥å‘é‡ æœ¬è«–æ–‡é è¨­ä½¿ç”¨ ==MEAN== ä½œç‚º pooling æ–¹æ³•  åˆ†é¡ä»»å‹™çš„æœ€ä½³åŒ–æ–¹å¼   $o=\text{softmax}(W_t(u,v,|u-v|))$</description>
    </item>
    
    <item>
      <title>Math Terms</title>
      <link>https://yingjialin.org/post/math-terms/</link>
      <pubDate>Tue, 02 Mar 2021 14:11:38 +0800</pubDate>
      
      <guid>https://yingjialin.org/post/math-terms/</guid>
      <description>æ•¸å­¸è­‰æ˜å¸¸ç”¨åè©  Definition (å®šç¾©): è§£é‡‹æŸå€‹æ•¸å­¸ç”¨èªçš„æ„ç¾© Theorem (å®šç†): é‡è¦çš„æ•¸å­¸æ•˜è¿°æˆ–ç†è«–ï¼Œé€šå¸¸ç¶“ç”±è­‰æ˜è€Œå¾— Axiom (å…¬ç†): æ™®éå¯ä»¥è¢«æ¥å—ä¸”ä¸ç”¨ç¶“éè­‰æ˜çš„ç¾è±¡æˆ–æ•˜è¿° Proposition (å‘½é¡Œ): å’Œå®šç†æ„æ¶µç›¸ä¼¼ï¼Œå¯èƒ½æ˜¯é‡è¦ç¨‹åº¦è¼ƒä½çš„å°çµæœ Lemma (å¼•ç†): è¼”åŠ©è­‰æ˜ä»¥å¼•å°å‡ºå®šç†çš„å°çµæœï¼Œå› ç‚ºè­‰æ˜å®šç†çš„éç¨‹å¯èƒ½ç¯‡å¹…å¾ˆé•· Remark (è©•è«–): å°æ–¼å®šç†æˆ–è­‰æ˜çµæœæ‰€å¼•ç”³å‡ºä¾†çš„æ•˜è¿° Corollary (æ¨è«–): åŸºæ–¼å®šç†ä¾†é€²ä¸€æ­¥æ¨å°çš„è«–è¿°ï¼Œé€šå¸¸ä¹Ÿæœ‰è­‰æ˜è¼”åŠ©  </description>
    </item>
    
  </channel>
</rss>
